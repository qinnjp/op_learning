## k8s

#### 1 k8s集群的安装

***1.1 k8s的架构***

![img](F:\typora\14791969222306.png)

![img](F:\typora\14791969311297.png)

- k8s架构
  - kubernetes Master （管理）
    - API Server
      - 开发或运维通过命令行控制来进行管理
    - etcd
      - 数据库，保存数据，属于nosql
      - 所有数据都是由API Server来写入的，其他人没有权限
    - Scheduler
      - 调度控制节点
    - Controller
      - 监控node节点上面的容器状态
      - 实现故障自愈
  - kubernetes Node （操作）
    - Kubelet
      - 创建容器的时候由Scheduler挑选节点，之后API Server调度Kubelet创建容器，但容器是由Kubelet通过docker创建的
    - CADviso
      - 监控容器状态，收集容器信息，现在已经集成在Kubelet
    - Kube-Porxy
      - 负载均衡，接入后端的pod，默认一人一次
    - Plugin NetWork
      - 负责节点之间的通信
      - 类型flannel（常用）
      - etc
  - 私有仓库

除了核心组件，还有一些推荐的Add-ons：

| **组件名称**              |           **说明**           |
| :------------------------ | :--------------------------: |
| **kube-dns**              |  负责为整个集群提供DNS服务   |
| **Ingress Controller**    |      为服务提供外网入口      |
| **Heapster**              |         提供资源监控         |
| **Dashboard**             |           提供GUI            |
| **Federation**            |      提供跨可用区的集群      |
| **Fluentd-elasticsearch** | 提供集群日志采集、存储与查询 |

这些服务都是运行在容器里面

***1.2 修改IP地址、主机和host解析***

```bash
10.0.0.11  k8s-master
10.0.0.12  k8s-node-1
10.0.0.13  k8s-node-2
```

所有节点需要做hosts解析

***1.3 master节点安装etcd***

```bash
#安装
yum install etcd -y
#配置
vim /etc/etcd/etcd.conf
6行：ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"
21行：ETCD_ADVERTISE_CLIENT_URLS="http://10.0.0.11:2379" 节点连接地址

#启动
systemctl start etcd.service
systemctl enable etcd.service

etcdctl set testdir/testkey0 0  #测试设置取值
etcdctl get testdir/testkey0

etcdctl -C http://10.0.0.11:2379 cluster-health	#跨界店取值
[root@k8s-master rc]# etcdctl -C http://10.0.0.11:2379 cluster-health
member 8e9e05c52164694d is healthy: got healthy result from http://10.0.0.11:2379
cluster is healthy

```

etcd原生支持做集群

https://www.jianshu.com/p/16c0c2b90e34

***1.4 master节点安装kubernetes***

```bash
#安装
yum install kubernetes-master.x86_64 -y
#配置
vim /etc/kubernetes/apiserver 
8行:  KUBE_API_ADDRESS="--insecure-bind-address=0.0.0.0"  #监听地址任意网段
11行：KUBE_API_PORT="--port=8080"
14行: KUBELET_PORT="--kubelet-port=10250"  
17行：KUBE_ETCD_SERVERS="--etcd-servers=http://10.0.0.11:2379"  #数据库地址
23行：KUBE_ADMISSION_CONTROL="--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ResourceQuota"

vim /etc/kubernetes/config
22行：KUBE_MASTER="--master=http://10.0.0.11:8080"  #master地址

#启动
systemctl enable kube-apiserver.service
systemctl restart kube-apiserver.service
systemctl enable kube-controller-manager.service
systemctl restart kube-controller-manager.service
systemctl enable kube-scheduler.service
systemctl restart kube-scheduler.service

#检测是否安装正常（查看组件状态）
[root@k8s-master rc]# kubectl get componentstatus 
NAME                 STATUS    MESSAGE             ERROR
etcd-0               Healthy   {"health":"true"}   
scheduler            Healthy   ok                 
controller-manager   Healthy   ok                 
```

***1.5 node节点安装kubernetes***

```bash
#安装
yum install kubernetes-node.x86_64 -y
#配置
vim /etc/kubernetes/config 
22行：KUBE_MASTER="--master=http://10.0.0.11:8080"

vim /etc/kubernetes/kubelet
5行：KUBELET_ADDRESS="--address=0.0.0.0"
8行：KUBELET_PORT="--port=10250"
11行：KUBELET_HOSTNAME="--hostname-override=10.0.0.12"
14行：KUBELET_API_SERVER="--api-servers=http://10.0.0.11:8080"
#启动
systemctl enable kubelet.service	
systemctl restart kubelet.service
#启动kubelet时会自动拉取docker
systemctl enable kube-proxy.service
systemctl restart kube-proxy.service

#在master节点检查
[root@k8s-master rc]# kubectl get nodes
NAME        STATUS    AGE
10.0.0.12   Ready     10h
10.0.0.13   Ready     10h

```

***1.6 所有节点配置flannel网络***

```bash
#安装
yum install flannel -y
#配置
sed -i 's#http://127.0.0.1:2379#http://10.0.0.11:2379#g' /etc/sysconfig/flanneld

##master节点：
etcdctl mk /atomic.io/network/config   '{ "Network": "172.18.0.0/16" }'
#查看
[root@k8s-master rc]# etcdctl ls /atomic.io/network/config
/atomic.io/network/config
[root@k8s-master rc]# etcdctl get /atomic.io/network/config
{ "Network": "172.18.0.0/16" }


yum install docker -y
systemctl enable flanneld.service 
systemctl restart flanneld.service 
systemctl  restart  docker
systemctl  enable  docker
systemctl restart kube-apiserver.service
systemctl restart kube-controller-manager.service
systemctl restart kube-scheduler.service

##node节点：
systemctl enable flanneld.service 
systemctl restart flanneld.service 
systemctl  restart  docker
systemctl restart kubelet.service
systemctl restart kube-proxy.service

#重启docker网络不通是因为
[root@k8s-master rc]# iptables -L
Chain FORWARD (policy ACCEPT)

vim /usr/lib/systemd/system/docker.service
#在[Service]区域下增加一行
ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT
systemctl daemon-reload 
systemctl restart docker
```

systemd详细介绍

 http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-part-two.html

***1.7 配置master为镜像仓库***

```bash
#所有节点
vi /etc/docker/daemon.json
{
"registry-mirrors": ["https://registry.docker-cn.com"],
"insecure-registries": ["10.0.0.11:5000"]
}
systemctl restart docker
#master节点
docker run -d -p 5000:5000 --restart=always --name registry -v /opt/myregistry:/var/lib/registry  registry
```

#### 2 什么是k8s,k8s有什么功能?

k8s是一个docker集群的管理工具

k8s是容器的编排工具

***2.1 k8s的核心功能***

- 自愈: 

  重新启动失败的容器，在节点不可用时，替换和重新调度节点上的容器，对用户定义的健康检查不响应的容器会被中止，并且在容器准备好服务之前不会把其向客户端广播。

- 弹性伸缩: 

  通过监控容器的cpu的负载值,如果这个平均高于80%,增加容器的数量,如果这个平均低于10%,减少容器的数量

- 服务的自动发现和负载均衡: 

  不需要修改您的应用程序来使用不熟悉的服务发现机制，Kubernetes 为容器提供了自己的 IP 地址和一组容器的单个 DNS 名称，并可以在它们之间进行负载均衡。

- 滚动升级和一键回滚: 

  Kubernetes 逐渐部署对应用程序或其配置的更改，同时监视应用程序运行状况，以确保它不会同时终止所有实例。 如果出现问题，Kubernetes会为您恢复更改，利用日益增长的部署解决方案的生态系统。

- 私密配置文件管理: 

  web容器里面,数据库的账户密码

***2.2 k8s的历史***

`2014年 docker容器编排工具，立项`

`2015年7月 发布kubernetes 1.0, 加入cncf基金会 孵化`

`2016年，kubernetes干掉两个对手，docker swarm，mesos marathon 1.2版`

`2017年 1.5 -1.9`

`2018年 k8s 从cncf基金会 毕业项目`

`2019年： 1.13, 1.14 ，1.15,1.16 1.17`

cncf :cloud native compute foundation

> kubernetes （k8s）: 希腊语 舵手，领航者 容器编排领域，谷歌15年容器使用经验，borg容器管理平台，使用golang重构borg，kubernetes

***2.3 k8s的安装方式***

- yum安装 1.5 最容易安装成功，最适合学习的
- 源码编译安装---难度最大 可以安装最新版
- 二进制安装---步骤繁琐 可以安装最新版 shell,ansible,saltstack
- kubeadm 安装最容易, 网络 可以安装最新版
- minikube 适合开发人员体验k8s, 网络

***2.4 k8s的应用场景***

k8s最适合跑微服务项目!

#### 3 k8s常用的资源

***3.1 创建pod资源***

`pod是最小资源单位.`

`任何的一个k8s资源都可以由yml清单文件来定义`

`k8s yaml的主要组成`

```bash
apiVersion: v1 		#api版本
kind: pod  			#资源类型
metadata:   		#属性
spec:       		#详细
```

k8s_pod.yaml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx	#资源名称
  labels:
    app: web	#标签
spec:
  containers:
    - name: nginx	
      image: 10.0.0.11:5000/nginx:1.13 #镜像
      ports:
        - containerPort: 80	#端口
```

- k8s常用命令

```bash
#创建资源
kubectl create -f k8s_pod.yml

#查看资源
kubectl get pod(资源类型)

#查看资源的调度节点
kubectl get pod -o wide

#查看资源的详细描述(排错)
kubectl describe pod(资源类型) nginx(名称)

#删除镜像
kubectl delete pod nginx
kubectl delete -f xxx.yaml

#在线修改配置文件
kubectl edit pod nginx

`排错`
#node节点:
vim /etc/kubernetes/kubelet
KUBELET_POD_INFRA_CONTAINER="--pod-infra-container-image=10.0.0.11:5000/rhel7/pod-infrastructure:latest"
systemctl restart kubelet.service
```

![1575899579103](F:\typora\1575899579103.png)

`为什么创建一个pod资源？k8s需要启两个容器`

- 业务容器
- 基础容器pod，定制化功能

pod资源:至少由两个容器组成,pod基础容器和业务容器组成(最多1+4)

pod配置文件2：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test
  labels:
    app: web
spec:
  containers:
    - name: nginx
      image: 10.0.0.11:5000/nginx:1.13
      ports:
        - containerPort: 80
    - name: busybox
      image: 10.0.0.11:5000/busybox:latest
      command: ["sleep","1000"]
```

pod是k8s最小的资源单位

- pod状态分析

```bash
CrashLoopBackOff： 容器退出，kubelet正在将它重启
InvalidImageName： 无法解析镜像名称
ImageInspectError： 无法校验镜像
ErrImageNeverPull： 策略禁止拉取镜像
ImagePullBackOff： 正在重试拉取
RegistryUnavailable： 连接不到镜像中心
ErrImagePull： 通用的拉取镜像出错
CreateContainerConfigError： 不能创建kubelet使用的容器配置
CreateContainerError： 创建容器失败
m.internalLifecycle.PreStartContainer  执行hook报错
RunContainerError： 启动容器失败
PostStartHookError： 执行hook报错 
ContainersNotInitialized： 容器没有初始化完毕
ContainersNotReady： 容器没有准备完毕 
ContainerCreating：容器创建中
PodInitializing：pod 初始化中 
DockerDaemonNotReady：docker还没有完全启动
NetworkPluginNotReady： 网络插件还没有完全启动
```



***3.2 ReplicationController资源***

rc:保证指定数量的pod始终存活,rc通过标签选择器来关联pod

创建一个rc

```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx
spec:
  replicas: 5  #副本5
  selector:
    app: myweb
  template:
    metadata:
      labels:
        app: myweb
    spec:
      containers:
      - name: myweb
        image: 10.0.0.11:5000/nginx:1.13
        ports:
        - containerPort: 80
```

```bash
#升级 
kubectl rolling-update nginx -f nginx-rc1.15.yaml --update-period=10s

#回滚 
kubectl rolling-update nginx2 -f nginx-rc.yaml --update-period=1s
```

deployment --all get

***3.3 service资源***

service帮助pod暴露端口

![1576396628230](F:\typora\1576396628230.png)

- 创建一个service资源

```yaml
apiVersion: v1
kind: Service
metadata:
  name: myweb
spec:
  type: NodePort  #ClusterIP
  ports:
    - port: 80          #clusterIP
      nodePort: 30000   #node port
      targetPort: 80    #pod port
  selector:      #标签选择器
    app: myweb2
    
    
#NodePort 端口映射(宿主机端口映射容器端口)
#ClusterIP (默认)只分配VIP不做端口映射
```

- 相关命令

```bash
#动态调整rc的副本数
kubectl scale rc nginx --replicas=2
kubectl exec -it pod_name /bin/bash
```

- 修改nodePort范围

```bash
vim  /etc/kubernetes/apiserver
KUBE_API_ARGS="--service-node-port-range=3000-50000"
```

- 命令行创建service资源

```bash
kubectl expose rc nginx --type=NodePort --port=80 --target-port=80
```

`service默认使用iptables来实现负载均衡, k8s 1.8新版本中推荐使用lvs(四层负载均衡 传输层tcp,udp)`

***3.4 deployment资源***

有rc在滚动升级之后,会造成服务访问中断,于是k8s引入了deployment资源

- 创建deployment

```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: 10.0.0.11:5000/nginx:1.13
        ports:
        - containerPort: 80
        resources:  #资源限制
          limits:    #最大值
            cpu: 100m
          requests:  #最小值
            cpu: 100m
```

- 创建svc

```bash
kubectl expose deployment nginx-deployment --type=NodePort --port=80 --target-port=80
```

- 升级/回退

```bash
#配置文件升级/回退
kubectl edit deployment nginx-deployment
#命令行升级/回退
##升级
kubectl set image deploy nginx nginx=10.0.0.11:5000/nginx:1.15
##回滚
kubectl rollout undo deployment nginx
##回滚到指定版本
kubectl rollout undo deployment nginx --to-revision=2

```

- 升级策略

```bash
kubectl edit deployment nginx-deployment
...
    minReadySeconds: 15  #升级间隔时间
    rollingUpdate:
      maxSurge: 1    #一次性升级的数量
      maxUnavailable: 1  #最大无效数
    type: RollingUpdate
...

```

https://www.jianshu.com/p/783c7a491b3d

- deployment 启动时创建rs资源
  - rc副本控制器
  - rs升级版副本控制器
- 区别：服务不中断
- 命令行创建deployment

```bash
kubectl run nginx --image=10.0.0.11:5000/nginx:1.13 --replicas=3 --record
```

- 查看deployment版本

```bash
kubectl rollout history deployment nginx-deployment
```

***3.5tomcat+mysql练习***

配置文件

```yaml
vi mysql-rc.yml
apiVersion: v1
kind: ReplicationController
metadata:
  name: mysql
spec:
  replicas: 1
  selector:
    app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
        - name: mysql
          image: 10.0.0.11:5000/mysql:5.7
          ports:
          - containerPort: 3306
          env:
          - name: MYSQL_ROOT_PASSWORD
            value: '123456'
```

- tomcat连接数据库

```yaml
vi mysql-svc.yml
apiVersion: v1
kind: Service
metadata:
  name: mysql
spec:
  ports:
    - port: 3306
      targetPort: 3306
  selector:
    app: mysql

```

- rc 配置文件

```yaml
vi tomcat-rc.yml
apiVersion: v1
kind: ReplicationController
metadata:
  name: myweb
spec:
  replicas: 1
  selector:
    app: myweb
  template:
    metadata:
      labels:
        app: myweb
    spec:
      containers:
        - name: myweb
          image: 10.0.0.11:5000/tomcat-app:v2
          ports:
          - containerPort: 8080
          env:
          - name: MYSQL_SERVICE_HOST
            value: 'mysql'
          - name: MYSQL_SERVICE_PORT
            value: '3306'

```

- tomcat 暴露端口（访问端口）

```yaml
vi tomcat-svc.yml
apiVersion: v1
kind: Service
metadata:
  name: myweb
spec:
  type: NodePort
  ports:
    - port: 8080
      nodePort: 30008
  selector:
    app: myweb

```

#### 4.k8s的附加组件

k8s集群中dns服务的作用,就是讲svc的名字解析成VIP地址

***4.1 dns服务***

安装dns服务

1:下载dns_docker镜像包

wget http://192.168.12.202/docker_image/docker_k8s_dns.tar.gz

2:导入dns_docker镜像包(node1节点)

3:修改skydns-rc.yaml, 在node1 创建dns服务

```yaml
  spec:
    nodeName: 10.0.0.12
```

4:创建dns服务

```bash
kubectl  create  -f   skydns-deploy.yaml
kubectl create -f skydns-svc.yaml
```

5:检查

```bash
kubectl get all --namespace=kube-system
```

6:修改所有node节点kubelet的配置文件

```bash
vim  /etc/kubernetes/kubelet

KUBELET_ARGS="--cluster_dns=10.254.230.254 --cluster_domain=cluster.local"

systemctl   restart kubelet
```

***4.2 namespace命名空间***

`namespace做资源隔离`

- 常用命令（默认将pod放在default下面）

```bash
#查看namespace
kubectl get namespace

#查看namespace下面的pod
kubectl get pod --namespace=default 

#创建
kubectl create namespace tomcat

#配置
[root@k8s-master tomcat_demo]# sed '3a \ \ namespace: tomcat' mysql-rc.yml | head -4
apiVersion: v1
kind: ReplicationController
metadata:
  namespace: tomcat
  
#查看tomcat namespace下面所有信息
kubectl get all -n tomcat

```

***4.3 健康检查***

*4.3.1 探针的种类*

- livenessProbe：健康状态检查，周期性检查服务是否存活，检查结果失败，将重启容器

- readinessProbe：可用性检查，周期性检查服务是否可用，不可用将从service的endpoints中移除

*4.3.2 探针的检测方法*

- exec：执行一段命令 返回值为0, 非0
- httpGet：检测某个 http 请求的返回状态码 2xx,3xx正常, 4xx,5xx错误
- tcpSocket：测试某个端口是否能够连接

*4.3.3 liveness探针的exec使用*

```yaml
vi  nginx_pod_exec.yaml 
iapiVersion: v1
kind: Pod
metadata:
  name: exec
spec:
  containers:
    - name: nginx
      image: 10.0.0.11:5000/nginx:1.13
      ports:
        - containerPort: 80
      args:
        - /bin/sh
        - -c
        - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
      livenessProbe:
        exec:
          command:
            - cat
            - /tmp/healthy
        initialDelaySeconds: 5   
        periodSeconds: 5
        timeoutSeconds: 5
        successThreshold: 1
        failureThreshold: 1
```

*4.3.4 liveness探针的httpGet使用*

```yaml
vi   nginx_pod_httpGet.yaml 
iapiVersion: v1
kind: Pod
metadata:
  name: httpget
spec:
  containers:
    - name: nginx
      image: 10.0.0.11:5000/nginx:1.13
      ports:
        - containerPort: 80
      livenessProbe:
        httpGet:
          path: /index.html
          port: 80
        initialDelaySeconds: 3
        periodSeconds: 3
```

*4.3.5 liveness探针的tcpSocket使用*

```yaml
vi   nginx_pod_tcpSocket.yaml
iapiVersion: v1
kind: Pod
metadata:
  name: tcpSocket
spec:
  containers:
    - name: nginx
      image: 10.0.0.11:5000/nginx:1.13
      ports:
        - containerPort: 80
      args:
        - /bin/sh
        - -c
        - tail -f /etc/hosts
      livenessProbe:
        tcpSocket:
          port: 80
        initialDelaySeconds: 10
        periodSeconds: 3
```

*4.3.6 readiness探针的httpGet使用*

```yaml
vi   nginx-rc-httpGet.yaml
iapiVersion: v1
kind: ReplicationController
metadata:
  name: readiness
spec:
  replicas: 2
  selector:
    app: readiness
  template:
    metadata:
      labels:
        app: readiness
    spec:
      containers:
      - name: readiness
        image: 10.0.0.11:5000/nginx:1.13
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:
            path: /qiangge.html
            port: 80
          initialDelaySeconds: 3
          periodSeconds: 3
          
#创建svc
kubectl expose rc readiness --port=80 --target-port=80 --type=NodePort
```

***4.4 dashboard服务***  

1. 上传并导入镜像,打标签
2. 创建dashborad的deployment和service
3. 访问http://10.0.0.11:8080/ui/

高级资源

- daemon sets（每个node节点启一个pod）

```yaml
[root@k8s-master deamonset]# cat k8s_deamon.yml 
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: nginx
spec:
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: 10.0.0.11:5000/nginx:1.13
        ports:
        - containerPort: 80
        resources:  
          limits:
            cpu: 100m
          requests:
            cpu: 100m
```

- pet sets     宠物应用	有状态的应用，有自己的数据
  - mysql主从，redis
- jobs  一次性容器

***4.5 通过apiservicer反向代理访问service***

```yaml
#第一种：NodePort类型 
  type: NodePort
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30008

#第二种：ClusterIP类型
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 80   

http://10.0.0.11:8080/api/v1/proxy/namespaces/命名空间/services/service的名字/

http://10.0.0.11:8080/api/v1/proxy/namespaces/qiangge/services/wordpress
```

***4.6 k8s配置docker镜像仓库密码***

```bash
kubectl create secret docker-registry default --docker-server=blog.oldqiang.com --docker-username=admin --docker-password=123456 --docker-email=123@qq.com
```

![1576069549693](F:\typora\1576069549693.png)

pod yaml文件帮助

`kubectl explain pod `  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    app: web
spec:
  imagePullSecrets:  #添加pull镜像的秘钥
    - name: default
  containers:
    - name: nginx
      image: blog.oldqiang.com/library/nginx:1.13
      ports:
        - containerPort: 80
```

#### 5: k8s弹性伸缩

`k8s弹性伸缩,需要附加插件heapster监控`

***5.1 安装heapster监控***

```bash
ls *.tar.gz
#导入镜像
for n in `ls *.tar.gz`;do docker load -i $n ;done
docker tag 

#打标签
docker.io/kubernetes/heapster_grafana:v2.6.0 10.0.0.11:5000/heapster_grafana:v2.6.0
docker tag  docker.io/kubernetes/heapster_influxdb:v0.5 10.0.0.11:5000/heapster_influxdb:v0.5
docker tag docker.io/kubernetes/heapster:canary 10.0.0.11:5000/heapster:canary
```

yaml文件

E:\docker相关文件\docker资料包\k8s_yaml\heapster\heapster-influxdb

![1576073646825](F:\typora\1576073646825.png)

- dashboard验证

![1576073977513](F:\typora\1576073977513.png)

***5.2 弹性伸缩***

![1576074351447](F:\typora\1576074351447.png)

配置文件

```yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-deployment
  namespace: default
spec:
  maxReplicas: 10
  minReplicas: 1
  scaleTargetRef:
    apiVersion: extensions/v1beta1
    kind: Deployment
    name: nginx-deployment
  targetCPUUtilizationPercentage: 5
  
  
#创建svc
kubectl expose rc readiness --port=80 --target-port=80 --type=NodePort
```

- 2.创建弹性伸缩规则

```bash
kubectl autoscale deploy nginx-deployment --max=8 --min=1 --cpu-percent=10
```

- 3.测试

```bash
ab -n 1000000 -c 40 http://172.16.28.6/index.html
```

#### 6 持久化存储

`数据持久化类型:`

***6.1 emptyDir:***

```bash
#查看yaml语法
kubectl explain pod
```



```yaml
   spec:
      nodeName: 10.0.0.13
      volumes:
      - name: mysql
        emptyDir: {}
      containers:
        - name: wp-mysql
          image: 10.0.0.11:5000/mysql:5.7
          imagePullPolicy: IfNotPresent
          ports:
          - containerPort: 3306
          volumeMounts:
          - mountPath: /var/lib/mysql
            name: mysql
```

***6.2 HostPath:***

```yaml
    spec:
      nodeName: 10.0.0.13
      volumes:
      - name: mysql
        hostPath:
          path: /data/wp_mysql
      containers:
        - name: wp-mysql
          image: 10.0.0.11:5000/mysql:5.7
          imagePullPolicy: IfNotPresent
          ports:
          - containerPort: 3306
          volumeMounts:
          - mountPath: /var/lib/mysql
            name: mysql
```

***6.3 nfs数据持久化***

- 安装配置nfs服务

```bash
#安装
yum install nfs-utils.x86_64 -y

#配置
mkdir /data
vim /etc/exports
/data  10.0.0.0/24(rw,async,no_root_squash,no_all_squash)
#启动
systemctl start rpcbind
systemctl start nfs
```

- 在node节点安装nfs客户端

```bash
yum install nfs-utils.x86_64 -y
showmount -e 10.0.0.11
```



```yaml
  spec:
      volumes:
      - name: mysql
        nfs:
          path: /data/wp_mysql
          server: 10.0.0.11
      containers:
        - name: mysql
          image: 10.0.0.11:5000/mysql:5.7
          imagePullPolicy: IfNotPresent
          ports:
          - containerPort: 3306
          volumeMounts:
          - mountPath: /var/lib/mysql
            name: mysql    #与volumes名称对应
           
```

***6.4 分布式存储glusterfs***

- 什么是glusterfs

  Glusterfs是一个开源分布式文件系统，具有强大的横向扩展能力，可支持数PB存储容量和数千客户端，通过网络互联成一个并行的网络文件系统。具有可扩展性、高性能、高可用性等特点。

- 安装glusterfs

```bash
#所有节点：

yum install  centos-release-gluster -y
yum install  install glusterfs-server -y

systemctl start glusterd.service
systemctl enable glusterd.service

mkdir -p /gfs/test1
mkdir -p /gfs/test2
mkdir -p /gfs/test3
```

- 添加存储资源池

```bash
#添加硬盘
#虚拟机热添加
echo '- - -' >/sys/class/scsi_host/host0/scan
echo '- - -' >/sys/class/scsi_host/host1/scan
echo '- - -' >/sys/class/scsi_host/host2/scan
#格式化，挂载
mkfs.xfs /dev/sdb 
mkfs.xfs /dev/sdc 
mkfs.xfs /dev/sdd
mount /dev/sdb /gfs/test1
mount /dev/sdc /gfs/test2
mount /dev/sdd /gfs/test3

#创建分布式卷
gluster volume create oldxu k8s-master:/gfs/test1 k8s-node-1:/gfs/test1 k8s-node-2:/gfs/test1 force
gluster volume info oldxu 
gluster volume start oldxu 
gluster volume info oldxu

#创建分布式复制卷
gluster volume add-brick oldxu replica 2 k8s-master:/gfs/test2 k8s-node-1:/gfs/test2 k8s-node-2:/gfs/test2 force

#扩容
gluster volume add-brick oldxu  k8s-master:/gfs/test3 k8s-node-1:/gfs/test3  force

#挂载
mount -t glusterfs 127.0.0.1:/oldxu /mnt

```

***6.6 k8s 对接glusterfs存储***

- 创建endpoint

```yaml
vi  glusterfs-ep.yaml
iapiVersion: v1
kind: Endpoints
metadata:
  name: glusterfs
  namespace: tomcat
subsets:
- addresses:
  - ip: 10.0.0.11
  - ip: 10.0.0.12
  - ip: 10.0.0.13
  ports:
  - port: 49152
    protocol: TCP
```

- 创建service

```yaml
vi  glusterfs-svc.yaml
iapiVersion: v1
kind: Service
metadata:
  name: glusterfs
  namespace: tomcat
spec:
  ports:
  - port: 49152
    protocol: TCP
    targetPort: 49152
  sessionAffinity: None
  type: ClusterIP
```

- rc引用

```yaml
    spec:
      volumes:
      - name: mysql
        glusterfs:  
          path: oldxu
          endpoints: glusterfs

```

***6.7 k8s 映射***

把外部的服务，通过创建service和endpoint，把他映射到k8s内部来使用。

- ep yaml文件

```yaml
[root@k8s-master wordpress]# cat mysql-ep.yaml 
apiVersion: v1
kind: Endpoints
metadata:
  name: mysql-wp
  namespace: wordpress
subsets:
- addresses:
  - ip: 10.0.0.13
  ports:
  - port: 3306
    protocol: TCP
```

- src

```yaml
[root@k8s-master wordpress]# cat mysql-svc.yml 
apiVersion: v1
kind: Service
metadata:
  namespace: wordpress
  name: mysql-wp
spec:
  ports:
    - port: 3306
      targetPort: 3306
```

***6.8 pvc***

- pv: persistent volume 全局资源,k8s集群

- pvc: persistent volume claim, 局部资源属于某一个namespace

> 全局资源：所有namespace 都能查看到的资源，node节点，pv，namespace
>
> 局部资源：只属于某一个具体namespacce下的资源

![1576150389344](F:\typora\1576150389344.png)

pv yaml文件

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv1
  labels:
    type: nfs
spec:
  capacity:
    storage: 10Gi 
  accessModes:
    - ReadWriteMany 
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: "/data/pv1"
    server: 10.0.0.11
    readOnly: false
###############################glusterfs
    glusterfs:
    path: oldxu
    endpoints: glusterfs

```

pvc yaml文件

```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc1
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 30Gi
```

```bash
#创建，查看
kubectl get pvc -n tomcat
```

rc配置

```yaml
volumes:
      - name: mysql
        persistentVolumeClaim: 
          claimName: pvc1
```

pvc关联pv

```yaml
#pv
...
metadata:
  name: pv1
  labels:
    pv: pv1
....
#pvc
...
spec: 
...
  selector: 
    matchLabels:
      pv: pv2
...
```





#### 6.为k8s集群配置dashboard服务

***1.kubeadm安装k8s1.15部署dashboard***

```bash
#下载dashboard yaml文件
wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml

#下载dashboard镜像
docker pull registry.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.10.1

#创建
kubectl create -f kubernetes-dashboard.yaml

#查看
kubectl get pod -n kube-system

#给node节点分配角色
kubectl label nodes kubernetes-node1 node-role.kubernetes.io/node=
##查看
kubectl get nodes --show-labels

#解决Google浏览器不能打开kubernetes dashboard方法
mkdir key && cd key
#生成证书
openssl genrsa -out dashboard.key 2048 
openssl req -new -out dashboard.csr -key dashboard.key -subj '/CN=10.0.0.11'
openssl x509 -req -in dashboard.csr -signkey dashboard.key -out dashboard.crt 
#删除原有的证书secret
kubectl delete secret kubernetes-dashboard-certs -n kube-system
#创建新的证书secret
kubectl create secret generic kubernetes-dashboard-certs --from-file=dashboard.key --from-file=dashboard.crt -n kube-system
#查看pod
kubectl get pod -n kube-system
#删除pod，启动新pod生效
kubectl delete  pod -n kube-sytem  kubernetes-dashboard-7c697b776b-zph98
#查看验证令牌

kubectl get secrets -n kube-system | grep dashboard
kubectl describe -n kube-system secrets kubernetes-dashboard-token-jwt67

#提升权限
##编辑文件vim k8s-admin.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: admin
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: admin
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin
  namespace: kube-system
  labels:
    kubernetes.io/cluster-service: "true"
##查看令牌
kubectl get secrets -n kube-system | grep admin
    

eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi10b2tlbi1mNGZrNCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJhZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjhiZGIwM2NkLWUwZjktNDZlYS1iMDBjLTYwNGRjNTE2NGUwYyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTphZG1pbiJ9.D83KOBSaqrPf7VtTQP7FJ-sxBbO9kWHkw5XqtK70VPhpZl_JMIyDLtH-1tVn2KPCDNCmdtZ5DKgdriEceUVeWADumKox_XqE74TWiDr8kJbb3_bEPeMv4hiHSTujcJ_1T59yRV5Hc8uR3ig4ABiG9nW3Z57qibyx4J2q4jUsKb1gUHWdFcMvIsPuTYaPTAha5O4EwbKj5mvjjq5HVD1I9NMsyQyx173ReoInqe3Q7VaVW1vE-_MPJbLFEiXaet_pj2F33u9poIwgE6s13-up2i6DoCfUq1mmlmsJyGSeD5603mmRze671d3g-6BkkEvBh-MkA9vigvA1WhkgbcdbZA
```

.....................................................................

`k8s学习网站`

https://www.qikqiak.com/k8s-book/docs/42.Helm%E5%AE%89%E8%A3%85.html